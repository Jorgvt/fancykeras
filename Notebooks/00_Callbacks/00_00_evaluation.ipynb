{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp callbacks.evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> Simple callbacks to evaluate the current training model on a dataset at different training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a class that stores a given dataset and calls evaluate on the training model when needed to obtain the evaluation metrics. We want it to be flexible in a way that we can specify a number of epochs or batches as the evaluation frequency. This could be solved by having a different callback for epoch and batches, but we probably can get away with using only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EvaluateDataset(Callback):\n",
    "    \"\"\"Evaluates a given `tf.data.Dataset` at different training times.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset, # Dataset to be evaluated.\n",
    "                 freq_epochs=None, # Number of epochs to wait between evaluations. `None` means not evaluating at an epoch interval.\n",
    "                 freq_batches=None, # Number of batches to wait between evaluations. `None` means not evaluating at a batch interval.\n",
    "                 append=\"\", # Text to append to the metrics' names as an identifier.\n",
    "                 ):\n",
    "        self.dataset = dataset if isinstance(dataset, tf.data.Dataset) else self._convert_to_dataset(dataset)\n",
    "        self.freq_epochs = freq_epochs\n",
    "        self.freq_batches = freq_batches\n",
    "        self.append = append\n",
    "        self.batches_seen, self.epochs_seen = 0, 0\n",
    "        self._results_batches, self._results_epochs = [], []\n",
    "        self.cuac, self.cuac2 = 0, 0\n",
    "    def _convert_to_dataset(self,\n",
    "                            dataset, # Dataset to be converted.\n",
    "                            ):\n",
    "        \"\"\"Tries to convert a dataset into a `tf.data.Dataset`.\"\"\"\n",
    "        return dataset\n",
    "\n",
    "    def evaluate(self,\n",
    "                 ) -> Dict: # Dictionary of evaluation results.\n",
    "        \"\"\"Calls the `.evaluate()` method of the given `model` on the stored `dataset`.\"\"\"\n",
    "        return {f\"{name}{self.append}\": value for name, value in self.model.evaluate(self.dataset, verbose=0, return_dict=True).items()}\n",
    "\n",
    "    def on_train_batch_end(self,\n",
    "                           batch, # Batch number in an epoch.\n",
    "                           logs=None, # Training logs.\n",
    "                           ):\n",
    "        self.cuac += 1\n",
    "        if self.freq_batches is None: \n",
    "            self.cuac2 += 1\n",
    "            return\n",
    "        else:\n",
    "            if self.batches_seen % self.freq_batches == 0: \n",
    "                results = self.evaluate()\n",
    "                self._results_batches.append(results)\n",
    "            self.batches_seen += 1\n",
    "    \n",
    "    def on_epoch_end(self,\n",
    "                     batch, # Batch number in an epoch.\n",
    "                     logs=None, # Training logs.\n",
    "                     ):\n",
    "        if self.freq_epochs is None: return\n",
    "        else:\n",
    "            if self.epochs_seen % self.freq_epochs == 0: \n",
    "                results = self.evaluate()\n",
    "                self._results_epochs.append(results)\n",
    "            self.epochs_seen += 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def _unpack_list_dicts(list_of_dicts):\n",
    "        \"\"\"Unpacks a list of dicts sharing keys into a dict with lists as values.\"\"\"\n",
    "        res = {}\n",
    "        for result in list_of_dicts:\n",
    "            for metric, value in result.items():\n",
    "                if metric not in res.keys(): res[metric] = []\n",
    "                res[metric].append(value)\n",
    "        return res\n",
    "\n",
    "    @property\n",
    "    def results_batches(self):\n",
    "        if len(self._results_batches) == 0: raise ValueError(\"No values stored yet.\")\n",
    "        return self._unpack_list_dicts(self._results_batches)\n",
    "\n",
    "    @property\n",
    "    def results_epochs(self):\n",
    "        if len(self._results_epochs) == 0: raise ValueError(\"No values stored yet.\")\n",
    "        return self._unpack_list_dicts(self._results_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "from iqadatasets.datasets.tid2013 import TID2013\n",
    "from iqadatasets.datasets.tid2008 import TID2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "tid13 = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2013\")\n",
    "tid08 = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality/TID/TID2008\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "from perceptnet.networks import PerceptNet\n",
    "from perceptnet.pearson_loss import PearsonCorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "model = PerceptNet(kernel_initializer=\"ones\", gdn_kernel_size=1, learnable_undersampling=False)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=PearsonCorrelation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "cb_eval = EvaluateDataset(tid13.dataset.batch(16).take(4), freq_batches=5, append=\"_TID2013\")\n",
    "history = model.fit(tid08.dataset.batch(16).take(10), epochs=2, callbacks=[cb_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_TID2013': [-0.9020472764968872,\n",
       "  -0.9032243490219116,\n",
       "  -0.9034193754196167,\n",
       "  -0.9044590592384338]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "cb_eval.results_batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf26')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
