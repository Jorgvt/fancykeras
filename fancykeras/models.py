# AUTOGENERATED! DO NOT EDIT! File to edit: ../Notebooks/03_Models/03_00_gradients.ipynb.

# %% auto 0
__all__ = ['ModelGrad', 'ModelGradSub']

# %% ../Notebooks/03_Models/03_00_gradients.ipynb 2
import tensorflow as tf
from tensorflow.keras import layers

from fastcore.basics import patch

# %% ../Notebooks/03_Models/03_00_gradients.ipynb 5
# @patch
def _pair_grads(#self: Model,
                model, # Model used to calculate the gradients.
                gradients, # Calculated gradients.
                ):
    return {var.name: tf.norm(grad, 2) for var, grad in zip(model.trainable_variables, gradients)}

# %% ../Notebooks/03_Models/03_00_gradients.ipynb 6
class ModelGrad(tf.keras.Model):
    """Keras model that logs the gradients during training."""

    def __init__(self,
                 inputs, # Inputs to the model.
                 outputs, # Outputs of the model.
                 **kwargs, # Key-word arguments to be passed to `tf.keras.Model`.
                 ):
        super(ModelGrad, self).__init__(inputs=inputs, outputs=outputs, **kwargs)

    def build(self,
             input_shape,
             ):
        super(ModelGrad, self).build(input_shape)
        self._build_metrics()

    def train_step(self,
                   data, # Data to perform a train step on.
                   ):
        X, Y = data
        with tf.GradientTape() as tape:
            pred = self(X, training=True)
            loss = self.compiled_loss(Y, pred)
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        paired_grads = _pair_grads(self, gradients)
        for m in self.grad_metrics:
            m.update_state(paired_grads[m.name[5:]])
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(Y, pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}


    @property
    def metrics(self):
        metrics = []
        if self._is_compiled:
            if self.compiled_loss is not None:
                metrics += self.compiled_loss.metrics
            if self.compiled_metrics is not None:
                metrics += self.compiled_metrics.metrics

        for l in self._flatten_layers():
            metrics.extend(l._metrics)
        
        metrics.extend(self.grad_metrics)
        return metrics

    def _build_metrics(self):
        self.grad_metrics = [tf.keras.metrics.Mean(name="grad_"+var.name) for var in self.trainable_variables]

# %% ../Notebooks/03_Models/03_00_gradients.ipynb 7
class ModelGradSub(tf.keras.Model):
    """Keras model that logs the gradients during training."""

    def __init__(self,
                #  inputs, # Inputs to the model.
                #  outputs, # Outputs of the model.
                 **kwargs, # Key-word arguments to be passed to `tf.keras.Model`.
                 ):
        super(ModelGradSub, self).__init__(**kwargs)

    def build(self,
             input_shape,
             ):
        super(ModelGradSub, self).build(input_shape)
        self._build_metrics()

    def train_step(self,
                   data, # Data to perform a train step on.
                   ):
        X, Y = data
        with tf.GradientTape() as tape:
            pred = self(X, training=True)
            loss = self.compiled_loss(Y, pred)
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        paired_grads = _pair_grads(self, gradients)
        for m in self.grad_metrics:
            m.update_state(paired_grads[m.name[5:]])
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(Y, pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}


    @property
    def metrics(self):
        metrics = []
        if self._is_compiled:
            if self.compiled_loss is not None:
                metrics += self.compiled_loss.metrics
            if self.compiled_metrics is not None:
                metrics += self.compiled_metrics.metrics

        for l in self._flatten_layers():
            metrics.extend(l._metrics)
        
        metrics.extend(self.grad_metrics)
        return metrics

    def _build_metrics(self):
        self.grad_metrics = [tf.keras.metrics.Mean(name="grad_"+var.name) for var in self.trainable_variables]
