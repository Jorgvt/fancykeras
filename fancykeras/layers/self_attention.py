# AUTOGENERATED! DO NOT EDIT! File to edit: ../../Notebooks/02_Layers/02_01_conv_self_attention.ipynb.

# %% auto 0
__all__ = ['SelfAttnConv']

# %% ../../Notebooks/02_Layers/02_01_conv_self_attention.ipynb 2
from einops import rearrange

import tensorflow as tf
from tensorflow.keras import layers

# %% ../../Notebooks/02_Layers/02_01_conv_self_attention.ipynb 4
class SelfAttnConv(layers.Layer):
    """Self-attention convolutional layer as implemented in SAGAN."""

    def __init__(self, 
                 emb_dim, # Embedding (channel) dim.
                 gamma=0.0, # Initial value of gamma.
                 return_attn=False, # Wether to return the attention map or not.
                 **kwargs, # Key-word arguments to be passed to the Conv2D layers.
                 ):
        super(SelfAttnConv, self).__init__()
        self.emb_dim = emb_dim
        self.gamma = gamma
        self.return_attn = return_attn
        self.query = layers.Conv2D(filters=emb_dim, kernel_size=1, **kwargs, name="query")
        self.key = layers.Conv2D(filters=emb_dim, kernel_size=1, **kwargs, name="key")
        self.value = layers.Conv2D(filters=emb_dim, kernel_size=1, **kwargs, name="value")
        
    def build(self,
             input_shape, # Input shape
             ):
        self.recover_channels = layers.Conv2D(filters=input_shape[0], kernel_size=1, name="recover_shape")
        self.gamma = tf.Variable(self.gamma, trainable=True, name="gamma")

    def call(self,
             inputs, # Inputs to the layer.
             ):
        ## 0. Store the shape of the input
        b, h, w, c = inputs.shape

        ## 1. Obtain the Query, Key, Value projections.
        query, key, value = self.query(inputs), self.key(inputs), self.value(inputs)

        ## 1.2. Flatten the spatial dims (and transpose the key).
        query = rearrange(tensor=query, pattern="b h w c -> b (h w) c")
        key = rearrange(tensor=key, pattern="b h w c -> b c (h w)")
        value = rearrange(tensor=value, pattern="b h w c -> b (h w) c")

        ##Â 2. Obtain the attention map by multiplying the Query and the Key and applying softmax.
        attn_map = tf.nn.softmax(query @ key, axis=-1)

        ## 3. Multiply the attention map with the value to obtain the output and recover the shape of the original image.
        output = attn_map @ value
        output = rearrange(tensor=output, pattern="b (h w) c -> b h w c", h=h, w=w)
        output = self.recover_channels(output)

        ## 4. Add the weighted value to the inputs with the corresponding global attention weight.
        output = inputs + self.gamma*output

        if self.return_attn: return attn_map, output
        else: return output
